# Petrosa Data Manager

**Data integrity, intelligence, and distribution hub for the Petrosa trading ecosystem**

The Data Manager ensures all trading-related datasets remain accurate, consistent, complete, and analyzable. It acts as both a guardian (maintaining data quality) and a gateway (serving structured data and analytics).

---

## ğŸŒ Overview

The Petrosa Data Manager is responsible for:

* **Data Integrity**: Continuous validation, gap detection, and consistency checking
* **Data Auditing**: Automated health scoring and quality monitoring
* **Data Recovery**: Intelligent backfilling of missing data
* **Analytics Computation**: Market metrics (volatility, volume, spread, trends, correlations)
* **Data Serving**: Schema-rich APIs for downstream consumption
* **Catalog Management**: Dataset registry, schemas, and lineage tracking

---

## ğŸ—ï¸ Architecture

### Core Components

| Component | Purpose |
|-----------|---------|
| **NATS Consumer** | Subscribe to `binance.futures.websocket.data` for real-time market data |
| **Auditor** | Validate data integrity, detect gaps, duplicates, and anomalies |
| **Backfiller** | Fetch and restore missing data ranges from Binance API |
| **Catalog** | Maintain dataset metadata, schemas, and lineage registry |
| **Analytics Engine** | Compute volatility, volume, spread, deviation, trend, seasonality metrics |
| **API Server** | RESTful endpoints for data access, metrics, health, and catalog |

### Data Flow

```
NATS: binance.futures.websocket.data
  â†“ (subscribe)
Data Manager Consumer
  â†“
Data Validation & Storage (PostgreSQL/MongoDB)
  â†“
Auditor (continuous) â†’ Backfiller (on gaps) â†’ Analytics (scheduled)
  â†“
API Layer (FastAPI) â†’ Downstream consumers (dashboards, strategies, tradeengine)
```

---

## ğŸš€ Quick Start

### Prerequisites

* Python 3.11+
* Docker
* kubectl (for Kubernetes deployment)
* Access to remote MicroK8s cluster

### Installation

```bash
# Complete setup
make setup

# Or manually
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements-dev.txt
```

### Local Development

```bash
# Run locally
make run

# Or directly
python -m data_manager.main
```

### Docker

```bash
# Build image
make build

# Run in Docker
make run-docker
```

### Kubernetes Deployment

```bash
# Deploy to cluster
make deploy

# Check status
make k8s-status

# View logs
make k8s-logs

# Clean up
make k8s-clean
```

---

## ğŸ“¡ API Endpoints

### Health & Status

* `GET /health/liveness` - Kubernetes liveness probe
* `GET /health/readiness` - Kubernetes readiness probe
* `GET /health/summary` - Overall system health
* `GET /health?pair={pair}&period={period}` - Data quality metrics

### Data Access

* `GET /data/candles?pair={pair}&period={period}` - OHLCV candle data
* `GET /data/trades?pair={pair}` - Individual trade data
* `GET /data/depth?pair={pair}` - Order book depth
* `GET /data/funding?pair={pair}` - Funding rate data

### Analytics

* `GET /analysis/volatility?pair={pair}&period={period}&method={method}` - Volatility metrics
* `GET /analysis/volume?pair={pair}&period={period}` - Volume metrics
* `GET /analysis/spread?pair={pair}` - Spread and liquidity
* `GET /analysis/trend?pair={pair}&period={period}` - Trend indicators
* `GET /analysis/correlation?pairs={pairs}&period={period}` - Correlation matrix

### Catalog

* `GET /catalog/datasets` - List all datasets
* `GET /catalog/datasets/{dataset_id}` - Dataset metadata
* `GET /catalog/schemas/{dataset_id}` - Schema definition
* `GET /catalog/lineage/{dataset_id}` - Data lineage

### Backfill

* `POST /backfill/start` - Trigger manual backfill
* `GET /backfill/jobs` - List backfill jobs
* `GET /backfill/jobs/{job_id}` - Job status

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `NATS_URL` | `nats://localhost:4222` | NATS server URL |
| `NATS_CONSUMER_SUBJECT` | `binance.futures.websocket.data` | NATS subject to subscribe |
| `POSTGRES_URL` | - | PostgreSQL connection string |
| `MONGODB_URL` | - | MongoDB connection string |
| `ENABLE_AUDITOR` | `true` | Enable data auditor |
| `ENABLE_BACKFILLER` | `true` | Enable backfiller |
| `ENABLE_ANALYTICS` | `true` | Enable analytics engine |
| `ENABLE_API` | `true` | Enable API server |
| `API_PORT` | `8000` | API server port |
| `AUDIT_INTERVAL` | `300` | Audit interval in seconds |
| `ANALYTICS_INTERVAL` | `900` | Analytics interval in seconds |
| `ENABLE_LEADER_ELECTION` | `true` | Enable MongoDB-based leader election |
| `LEADER_ELECTION_HEARTBEAT_INTERVAL` | `10` | Leader heartbeat interval (seconds) |
| `LEADER_ELECTION_TIMEOUT` | `30` | Leader election timeout (seconds) |
| `ENABLE_AUTO_BACKFILL` | `false` | Enable automatic backfill for detected gaps |
| `MIN_AUTO_BACKFILL_GAP` | `3600` | Minimum gap size to trigger backfill (seconds) |
| `MAX_AUTO_BACKFILL_JOBS` | `5` | Maximum concurrent backfill jobs |
| `ENABLE_DUPLICATE_REMOVAL` | `false` | Enable automatic duplicate removal |
| `DUPLICATE_RESOLUTION_STRATEGY` | `keep_newest` | Duplicate resolution strategy |

### Kubernetes Configuration

The service uses existing shared secrets and configmaps:

* **Secret**: `petrosa-sensitive-credentials` (database credentials)
* **ConfigMap**: `petrosa-common-config` (shared settings)
* **ConfigMap**: `petrosa-data-manager-config` (service-specific)

---

## ğŸ§ª Development

### Code Quality

```bash
# Run linters
make lint

# Format code
make format

# Run tests
make test

# Security scan
make security
```

### Complete Pipeline

```bash
# Run all checks
make pipeline
```

---

## ğŸ“Š Metrics

Prometheus metrics are exposed on port 9090:

* `data_manager_messages_received_total` - Total messages received from NATS
* `data_manager_messages_processed_total` - Successfully processed messages
* `data_manager_messages_failed_total` - Failed messages
* `data_manager_message_processing_seconds` - Message processing time
* `data_manager_nats_connection_status` - NATS connection status

---

## ğŸ—‚ï¸ Project Structure

```
petrosa-data-manager/
â”œâ”€â”€ data_manager/
â”‚   â”œâ”€â”€ models/          # Pydantic data models
â”‚   â”œâ”€â”€ consumer/        # NATS consumer and message handling
â”‚   â”œâ”€â”€ auditor/         # Data integrity validation
â”‚   â”œâ”€â”€ backfiller/      # Gap recovery and backfilling
â”‚   â”œâ”€â”€ catalog/         # Dataset registry and metadata
â”‚   â”œâ”€â”€ analytics/       # Metrics computation
â”‚   â”œâ”€â”€ api/             # FastAPI endpoints
â”‚   â”‚   â””â”€â”€ routes/      # API route modules
â”‚   â””â”€â”€ main.py          # Application entry point
â”œâ”€â”€ k8s/                 # Kubernetes manifests
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ constants.py         # Configuration constants
â”œâ”€â”€ otel_init.py         # OpenTelemetry initialization
â”œâ”€â”€ Dockerfile           # Container image
â”œâ”€â”€ Makefile             # Development commands
â””â”€â”€ README.md            # This file
```

---

## ğŸ”— Integration

### Event Bus (NATS)

The Data Manager subscribes to:

* `binance.futures.websocket.data` - Real-time market data from socket-client

Supported event types:

* `trade` - Individual trades
* `ticker` - 24h ticker statistics
* `depth` - Order book depth updates
* `markPrice` - Mark price updates
* `fundingRate` - Funding rate updates
* `kline` - Candle/kline data

### Databases

* **PostgreSQL**: Metadata, catalog, audit logs, health metrics
* **MongoDB**: Time series data (candles, trades, depth), computed metrics

---

## ğŸ”’ Leader Election

The Data Manager uses MongoDB-based leader election to ensure only one pod runs background schedulers (auditor, analytics) in multi-replica deployments.

### How It Works

1. **Election**: On startup, each pod attempts to become leader via atomic MongoDB write
2. **Heartbeat**: Leader sends heartbeat every 10 seconds to prove it's alive
3. **Failover**: If leader fails (no heartbeat for 30s), followers elect new leader
4. **Safety**: Prevents duplicate work and database contention

### Configuration

```yaml
# Enable leader election (recommended for production)
ENABLE_LEADER_ELECTION: "true"

# Heartbeat frequency
LEADER_ELECTION_HEARTBEAT_INTERVAL: "10"  # seconds

# Leader timeout
LEADER_ELECTION_TIMEOUT: "30"  # seconds
```

### Monitoring

```bash
# Check which pod is the leader
kubectl exec -it data-manager-xxx -- curl localhost:8000/health/leader

# Check audit scheduler status
kubectl exec -it data-manager-xxx -- curl localhost:8000/health/audit-status
```

See [docs/AUDITOR.md](docs/AUDITOR.md) for complete details.

---

## ğŸ¯ Roadmap

* âœ… NATS consumer for market data events
* âœ… FastAPI serving layer with schema-rich endpoints
* âœ… Kubernetes manifests and deployment
* âœ… Auditor implementation with leader election
  * âœ… Gap detection with auto-backfill integration
  * âœ… Duplicate detection and removal
  * âœ… Health scoring with enhanced metrics
  * âœ… MongoDB-based leader election for multi-replica safety
* âœ… Leader election for background schedulers
* ğŸš§ Database integration (PostgreSQL + MongoDB)
* ğŸš§ Backfiller implementation (Binance API integration)
* ğŸš§ Analytics engine (all metric calculators)
* ğŸš§ Catalog management (dataset registry)
* ğŸš§ Comprehensive test suite
* ğŸš§ CI/CD pipeline

---

## ğŸ“š Documentation

* **API Documentation**: Available at `/docs` when running (Swagger UI)
* **Metrics**: Available at `/metrics` (Prometheus format)
* **Health**: Available at `/health/*` endpoints

---

## ğŸ› ï¸ Troubleshooting

### NATS Connection Issues

```bash
# Check NATS connectivity
kubectl --kubeconfig=k8s/kubeconfig.yaml -n nats get pods

# View logs
make k8s-logs
```

### Database Connection Issues

```bash
# Verify secrets are configured
kubectl --kubeconfig=k8s/kubeconfig.yaml -n petrosa-apps get secret petrosa-sensitive-credentials
```

### API Not Responding

```bash
# Check pod status
make k8s-status

# Check readiness
curl http://petrosa-data-manager.petrosa-apps/health/readiness
```

---

## ğŸ“ License

MIT License - Petrosa Systems

---

## ğŸ‘¥ Authors

Petrosa Systems - Trading Infrastructure Team

